# [Attribution Preservation in Network Compression for Reliable Network Interpretation](https://arxiv.org/pdf/2010.15054v1.pdf)

**Problem**: Modern neural network architectures are hard to train due to their size in memory and number of FLOPs. 

**Solution**:
The authors propose three techniques to

**Notes**
* *Model compression* refers to any activity that reduces the size of the network while maintaining the predictive performance of the network within a certain acceptable margin (pruning, distillation, quantization, and more).

![Attribution preservation](../images/results_preservation.png)
